import pandas as pd
import numpy as np
import faiss
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib
import streamlit as st
import json
import os
import requests
import zipfile
from io import BytesIO
from typing import List, Dict, Tuple
from encoder import EmbeddingConfig
from loguru import logger
from config import config
import time

@st.cache_data(ttl=3600, show_spinner=False) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
def load_db_from_github(zip_url: str):
    """
    GitHub Releasesã‹ã‚‰zipã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    logger.info(f"â¬‡ï¸ GitHub Releasesã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {zip_url}")
    try:
        response = requests.get(zip_url)
        response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ

        with zipfile.ZipFile(BytesIO(response.content)) as z:
            # zipãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç‰¹å®š
            parquet_filename = next(name for name in z.namelist() if name.endswith('processed_data.parquet'))
            faiss_filename = next(name for name in z.namelist() if name.endswith('faiss_index.bin'))

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§èª­ã¿è¾¼ã‚€
            with z.open(parquet_filename) as pf:
                df = pd.read_parquet(pf)
            
            # group_codeåˆ—ã‚’è¿½åŠ ï¼ˆkoumoku_codeã®å…ˆé ­5æ–‡å­—ï¼‰
            if 'koumoku_code' in df.columns:
                # koumoku_codeã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›ã—ã¦ã‹ã‚‰å…ˆé ­5æ–‡å­—ã‚’å–å¾—
                df['group_code'] = df['koumoku_code'].astype(str).str[:5]
                logger.info(f"âœ… group_codeåˆ—ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆ{df['group_code'].nunique()}å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰")
            else:
                logger.warning("âš ï¸ koumoku_codeåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            with z.open(faiss_filename) as ff:
                # faissã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™
                temp_faiss_path = "temp_faiss_index.bin"
                with open(temp_faiss_path, "wb") as f_out:
                    f_out.write(ff.read())
                faiss_index = faiss.read_index(temp_faiss_path)
                os.remove(temp_faiss_path) # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤

        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨èª­ã¿è¾¼ã¿ãŒå®Œäº†")
        return df, faiss_index

    except Exception as e:
        logger.error(f"âŒ GitHubã‹ã‚‰ã®DBãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

class HybridRetriever:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ã¨ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.df = None
        self.faiss_index = None # ä»¥å‰ã¯ `index` ã ã£ãŸã‚‚ã®ã‚’ `faiss_index` ã«çµ±ä¸€
        self.bm25 = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.embedding_config = EmbeddingConfig()

    def load_vector_database(self) -> bool:
        """GitHub Releasesã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚€"""
        if self.df is not None and self.faiss_index is not None:
            return True

        # Config ç®¡ç†ã® ZIP URLï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
        zip_url = config.get_vector_db_zip_url()
        
        # GitHubã‹ã‚‰DBã‚’ãƒ­ãƒ¼ãƒ‰
        self.df, self.faiss_index = load_db_from_github(zip_url)

        if self.df is None or self.faiss_index is None:
            return False

        self._build_keyword_indices()
        logger.info("ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return True
    
    def _build_keyword_indices(self):
        """BM25/TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã€‚ãªã‘ã‚Œã°åˆå›ã®ã¿ä½œæˆã—ã¦æ°¸ç¶šåŒ–ã€‚"""
        try:
            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'vector_db'))
            os.makedirs(base_dir, exist_ok=True)
            bm25_path = os.path.join(base_dir, 'bm25.joblib')
            tfidf_path = os.path.join(base_dir, 'tfidf.joblib')

            loaded_any = False
            if os.path.exists(bm25_path):
                self.bm25 = joblib.load(bm25_path)
                loaded_any = True
            if os.path.exists(tfidf_path):
                tfidf_bundle = joblib.load(tfidf_path)
                self.tfidf_vectorizer = tfidf_bundle.get('vectorizer')
                self.tfidf_matrix = tfidf_bundle.get('matrix')
                loaded_any = True

            if not loaded_any:
                logger.info("ğŸ”§ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªæ¤œå‡ºã®ãŸã‚åˆå›æ§‹ç¯‰ã‚’è¡Œã„ã¾ã™â€¦")
                # æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿ç«‹ã¦ï¼ˆä»¥å‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¸è¥²ï¼‰
                search_texts = []
                for _, row in self.df.iterrows():
                    text = f"{row['koumoku_name_full']} {row['bunya_name']} {row['chuubunrui_name']} {row['shoubunrui_name']} {row['definition']} {row['stat_name']}"
                    search_texts.append(text)

                # BM25
                tokenized_texts = [text.split() for text in search_texts]
                self.bm25 = BM25Okapi(tokenized_texts)

                # TF-IDF
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=10000,
                    ngram_range=(1, 2),
                    stop_words=None
                )
                self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(search_texts)

                # æ°¸ç¶šåŒ–
                try:
                    joblib.dump(self.bm25, bm25_path)
                    joblib.dump({
                        'vectorizer': self.tfidf_vectorizer,
                        'matrix': self.tfidf_matrix
                    }, tfidf_path)
                    logger.info("âœ… BM25/TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚æ¬¡å›ä»¥é™ã®èµ·å‹•ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚")
                except Exception as persist_err:
                    logger.warning(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜ã«å¤±æ•—: {persist_err}")
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿/æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def vector_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        if self.faiss_index is None or self.embedding_config is None:
            logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¾ãŸã¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šãŒæœªåˆæœŸåŒ–")
            return []

        query_embedding = self.embedding_config.get_embeddings([query])

        if query_embedding.size == 0:
            logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return []

        # FAISSæ¤œç´¢
        distances, indices = self.faiss_index.search(query_embedding, top_k)
            
        # çµæœã‚’æ•´å½¢
        results = []
        for i, (score, idx) in enumerate(zip(distances[0], indices[0])):
            if idx != -1:  # æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                results.append((idx, score))
            
        logger.info(f"ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
        logger.debug(f"â± vector_search took {(time.time()-start_time)*1000:.1f} ms")
        return results
    
    def keyword_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        try:
            if self.bm25 is None:
                logger.error("âŒ BM25æ¤œç´¢: BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªåˆæœŸåŒ–")
                return []
                
            query_tokens = query.split()
            bm25_scores = self.bm25.get_scores(query_tokens)
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            indexed_scores = [(i, score) for i, score in enumerate(bm25_scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            results = indexed_scores[:top_k]
            logger.info(f"ğŸ” BM25æ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            logger.debug(f"â± keyword_search took {(time.time()-start_time)*1000:.1f} ms")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def tfidf_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """TF-IDFæ¤œç´¢ã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        try:
            if self.tfidf_vectorizer is None or self.tfidf_matrix is None:
                logger.error("âŒ TF-IDFæ¤œç´¢: TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªåˆæœŸåŒ–")
                return []
                
            query_vector = self.tfidf_vectorizer.transform([query])
            cosine_scores = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
            
            indexed_scores = [(i, score) for i, score in enumerate(cosine_scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            results = indexed_scores[:top_k]
            logger.info(f"ğŸ” TF-IDFæ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            logger.debug(f"â± tfidf_search took {(time.time()-start_time)*1000:.1f} ms")
            return results
            
        except Exception as e:
            logger.error(f"âŒ TF-IDFæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"TF-IDFæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def rerank_results(self, query: str, candidate_indices: List[int], top_k: int = 50) -> List[int]:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆNumPy/Pandaså¯„ã‚Šã«è»½é‡åŒ–ï¼‰ã€‚"""
        start_time = time.time()
        try:
            query_lower = query.lower()
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæœ€ä½é™ã®é«˜é€ŸåŒ–ï¼‰
            text_cols = ['koumoku_name_full', 'bunya_name', 'chuubunrui_name', 'shoubunrui_name', 'definition', 'stat_name']
            # å¯¾è±¡è¡Œã ã‘æŠ½å‡º
            sub_df = self.df.iloc[candidate_indices][text_cols].fillna("").astype(str)
            # å®Œå…¨ä¸€è‡´: å„åˆ—ã« query ã‚’å«ã‚€ã‹
            contain_mask = sub_df.applymap(lambda x: query_lower in x.lower())
            contain_score = contain_mask.sum(axis=1) * 2
            # å˜èªä¸€è‡´: å˜ç´”å˜èªåˆ†å‰²ã—ã¦é›†åˆç©
            q_words = set(query_lower.split())
            def word_overlap_score(row: pd.Series) -> int:
                s = 0
                for v in row.values:
                    fw = set(str(v).lower().split())
                    s += len(q_words & fw)
                return s
            overlap_score = sub_df.apply(word_overlap_score, axis=1)
            total = contain_score.add(overlap_score)
            # ã‚¹ã‚³ã‚¢é †
            order = total.sort_values(ascending=False).index
            # sub_df.index ã¯å…ƒDataFrameã®ãƒ©ãƒ™ãƒ«ã€‚ãƒ©ãƒ™ãƒ«â†’å€™è£œé…åˆ—å†…ã®ä½ç½®ã‚’ä½œã‚‹
            index_to_pos = {label: pos for pos, label in enumerate(sub_df.index)}
            ordered_candidate_indices = [candidate_indices[index_to_pos[label]] for label in order if label in index_to_pos][:top_k]
            logger.debug(f"â± rerank_results took {(time.time()-start_time)*1000:.1f} ms")
            return ordered_candidate_indices
        except Exception as e:
            st.error(f"ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return candidate_indices[:top_k]
    
    def hybrid_search(self, query: str, top_k: int = 50, vector_weight: float = 0.6) -> List[Dict]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆæ¤œç´¢ã‚’ä¸¦åˆ—åŒ–ã€çµåˆå‡¦ç†ã‚’åŠ¹ç‡åŒ–ï¼‰ã€‚"""
        total_start = time.time()
        try:
            logger.info(f"ğŸ” ã‚¯ã‚¨ãƒª: '{query}' (top_k={top_k})")

            # ä¸¦åˆ—å®Ÿè¡Œï¼ˆåŒæœŸI/Oã®ãŸã‚ThreadPoolã§ååˆ†ï¼‰
            from concurrent.futures import ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=3) as ex:
                f_vec = ex.submit(self.vector_search, query, top_k * 2)
                f_bm25 = ex.submit(self.keyword_search, query, top_k * 2)
                f_tfidf = ex.submit(self.tfidf_search, query, top_k * 2)
                vector_results = f_vec.result()
                bm25_results = f_bm25.result()
                tfidf_results = f_tfidf.result()
            
            logger.info(f"ğŸ“Š æ¤œç´¢çµæœæ•°: ãƒ™ã‚¯ãƒˆãƒ«={len(vector_results)}, BM25={len(bm25_results)}, TF-IDF={len(tfidf_results)}")
            
            # ã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–ã—ã¦ãƒãƒ¼ã‚¸
            all_candidates = {}
            keyword_weight = (1 - vector_weight) / 2
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ
            for idx, score in vector_results:
                all_candidates[idx] = all_candidates.get(idx, 0) + score * vector_weight
            
            # BM25çµæœ
            max_bm25 = max([score for _, score in bm25_results], default=1)
            for idx, score in bm25_results:
                normalized_score = score / max_bm25 if max_bm25 > 0 else 0
                all_candidates[idx] = all_candidates.get(idx, 0) + normalized_score * keyword_weight
            
            # TF-IDFçµæœ
            max_tfidf = max([score for _, score in tfidf_results], default=1)
            for idx, score in tfidf_results:
                normalized_score = score / max_tfidf if max_tfidf > 0 else 0
                all_candidates[idx] = all_candidates.get(idx, 0) + normalized_score * keyword_weight
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆNumPyã§é«˜é€ŸåŒ–ï¼‰
            import numpy as _np
            if all_candidates:
                idxs = _np.fromiter(all_candidates.keys(), dtype=_np.int64)
                vals = _np.fromiter(all_candidates.values(), dtype=_np.float32)
                order = _np.argsort(vals)[::-1]
                candidate_indices = idxs[order][: top_k * 2].tolist()
            else:
                candidate_indices = []
            
            logger.info(f"ğŸ”„ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å‰ã®å€™è£œæ•°: {len(candidate_indices)}")
            
            final_top_k = min(top_k, 40)  
            reranked_indices = self.rerank_results(query, candidate_indices, final_top_k)
            
            logger.info(f"âœ… ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¾Œã®çµæœæ•°: {len(reranked_indices)} (æœ€å¤§{final_top_k})")
            
            # --- ã“ã“ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å‡¦ç† ---
            results = []
            processed_groups = set() # å‡¦ç†æ¸ˆã¿ã®group_codeã‚’è¨˜éŒ²
            bunya_counts = {}
            
            for idx in reranked_indices:
                item = self.df.iloc[idx]
                group_code = item.get('group_code')

                if group_code not in processed_groups:
                    bunya = item['bunya_name']
                    bunya_counts[bunya] = bunya_counts.get(bunya, 0) + 1
                    
                    results.append({
                        'koumoku_name_full': item['koumoku_name_full'],
                        'bunya_name': item['bunya_name'],
                        'chuubunrui_name': item['chuubunrui_name'],
                        'shoubunrui_name': item['shoubunrui_name'],
                        'koumoku_code': item.get('koumoku_code', ''),
                        'group_code': group_code,
                        'score': all_candidates.get(idx, 0)
                    })
                    
                    processed_groups.add(group_code)

            logger.info(f"âš™ï¸ ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å¾Œã®æœ€çµ‚çµæœæ•°: {len(results)}ä»¶")
            # --- ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å‡¦ç†ã“ã“ã¾ã§ ---
            
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœã®åˆ†é‡åˆ†å¸ƒ: {dict(bunya_counts)}")
            logger.info(f"ğŸ¯ æ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®æŒ‡æ¨™ã‚’è¿”å´")
            
            logger.debug(f"â± hybrid_search total took {(time.time()-total_start)*1000:.1f} ms")
            return results
            
        except Exception as e:
            st.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
retriever = HybridRetriever() 