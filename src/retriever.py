import pandas as pd
import numpy as np
import faiss
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st
import json
import os
import requests
import zipfile
from io import BytesIO
from typing import List, Dict, Tuple
from encoder import EmbeddingConfig
from loguru import logger

@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
def load_db_from_github(zip_url: str):
    """
    GitHub Releasesã‹ã‚‰zipã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    logger.info(f"â¬‡ï¸ GitHub Releasesã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {zip_url}")
    try:
        response = requests.get(zip_url)
        response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ

        with zipfile.ZipFile(BytesIO(response.content)) as z:
            # zipãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç‰¹å®š
            parquet_filename = next(name for name in z.namelist() if name.endswith('processed_data.parquet'))
            faiss_filename = next(name for name in z.namelist() if name.endswith('faiss_index.bin'))

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§èª­ã¿è¾¼ã‚€
            with z.open(parquet_filename) as pf:
                df = pd.read_parquet(pf)
            
            # group_codeåˆ—ã‚’è¿½åŠ ï¼ˆkoumoku_codeã®å…ˆé ­5æ–‡å­—ï¼‰
            if 'koumoku_code' in df.columns:
                df['group_code'] = df['koumoku_code'].astype(str).str[:5]
                logger.info(f"âœ… group_codeåˆ—ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆ{df['group_code'].nunique()}å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰")
            else:
                logger.warning("âš ï¸ koumoku_codeåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            with z.open(faiss_filename) as ff:
                # faissã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™
                temp_faiss_path = "temp_faiss_index.bin"
                with open(temp_faiss_path, "wb") as f_out:
                    f_out.write(ff.read())
                faiss_index = faiss.read_index(temp_faiss_path)
                os.remove(temp_faiss_path) # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤

        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨èª­ã¿è¾¼ã¿ãŒå®Œäº†")
        return df, faiss_index

    except Exception as e:
        logger.error(f"âŒ GitHubã‹ã‚‰ã®DBãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None, None

class HybridRetriever:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ã¨ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.df = None
        self.faiss_index = None # ä»¥å‰ã¯ `index` ã ã£ãŸã‚‚ã®ã‚’ `faiss_index` ã«çµ±ä¸€
        self.bm25 = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.embedding_config = EmbeddingConfig()

    def load_vector_database(self) -> bool:
        """GitHub Releasesã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚€"""
        if self.df is not None and self.faiss_index is not None:
            return True

        # è‡ªèº«ã®GitHub Releasesã®URLã«æ›¸ãæ›ãˆã¦ãã ã•ã„
        zip_url = "https://github.com/hrkzz/japan_dashboard_stat_search/releases/download/v1.0.0/vector_db.zip"
        
        # GitHubã‹ã‚‰DBã‚’ãƒ­ãƒ¼ãƒ‰
        self.df, self.faiss_index = load_db_from_github(zip_url)

        if self.df is None or self.faiss_index is None:
            return False

        self._build_keyword_indices()
        logger.info("ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return True
    
    def _build_keyword_indices(self):
        """BM25ã¨TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        try:
            # æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
            search_texts = []
            for _, row in self.df.iterrows():
                text = f"{row['koumoku_name_full']} {row['bunya_name']} {row['chuubunrui_name']} {row['shoubunrui_name']} {row['definition']} {row['stat_name']}"
                search_texts.append(text)
            
            # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            tokenized_texts = [text.split() for text in search_texts]
            self.bm25 = BM25Okapi(tokenized_texts)
            
            # TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),
                stop_words=None  # æ—¥æœ¬èªå¯¾å¿œã®ãŸã‚
            )
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(search_texts)
            
        except Exception as e:
            st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def vector_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        if self.faiss_index is None or self.embedding_config is None:
            logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¾ãŸã¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šãŒæœªåˆæœŸåŒ–")
            return []

        query_embedding = self.embedding_config.get_embeddings([query])

        if query_embedding.size == 0:
            logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return []

        # FAISSæ¤œç´¢
        distances, indices = self.faiss_index.search(query_embedding, top_k)
            
        # çµæœã‚’æ•´å½¢
        results = []
        for i, (score, idx) in enumerate(zip(distances[0], indices[0])):
            if idx != -1:  # æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                results.append((idx, score))
            
        logger.info(f"ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
        return results
    
    def keyword_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            if self.bm25 is None:
                logger.error("âŒ BM25æ¤œç´¢: BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªåˆæœŸåŒ–")
                return []
                
            query_tokens = query.split()
            bm25_scores = self.bm25.get_scores(query_tokens)
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            indexed_scores = [(i, score) for i, score in enumerate(bm25_scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            results = indexed_scores[:top_k]
            logger.info(f"ğŸ” BM25æ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def tfidf_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """TF-IDFæ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            if self.tfidf_vectorizer is None or self.tfidf_matrix is None:
                logger.error("âŒ TF-IDFæ¤œç´¢: TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªåˆæœŸåŒ–")
                return []
                
            query_vector = self.tfidf_vectorizer.transform([query])
            cosine_scores = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
            
            indexed_scores = [(i, score) for i, score in enumerate(cosine_scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            results = indexed_scores[:top_k]
            logger.info(f"ğŸ” TF-IDFæ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            return results
            
        except Exception as e:
            logger.error(f"âŒ TF-IDFæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"TF-IDFæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def rerank_results(self, query: str, candidate_indices: List[int], top_k: int = 50) -> List[int]:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰"""
        try:
            query_lower = query.lower()
            scored_candidates = []
            
            for idx in candidate_indices:
                row = self.df.iloc[idx]
                
                # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã®ãƒãƒƒãƒãƒ³ã‚°åº¦ã‚’è¨ˆç®—
                score = 0
                text_fields = [
                    row['koumoku_name_full'],
                    row['bunya_name'],
                    row['chuubunrui_name'],
                    row['shoubunrui_name'],
                    row['definition'],
                    row['stat_name']
                ]
                
                for field in text_fields:
                    if pd.isna(field):
                        continue
                    field_lower = str(field).lower()
                    
                    # å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
                    if query_lower in field_lower:
                        score += 2
                    
                    # éƒ¨åˆ†ä¸€è‡´
                    query_words = query_lower.split()
                    field_words = field_lower.split()
                    matches = len(set(query_words) & set(field_words))
                    score += matches
                
                scored_candidates.append((idx, score))
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™
            scored_candidates.sort(key=lambda x: x[1], reverse=True)
            return [idx for idx, _ in scored_candidates[:top_k]]
            
        except Exception as e:
            st.error(f"ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return candidate_indices[:top_k]
    
    def hybrid_search(self, query: str, top_k: int = 50, vector_weight: float = 0.6) -> List[Dict]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸ” ã‚¯ã‚¨ãƒª: '{query}' (top_k={top_k})")
            
            # å„æ¤œç´¢æ‰‹æ³•ã‚’å®Ÿè¡Œ
            # å„æ¤œç´¢æ‰‹æ³•ã§çµæœã‚’å–å¾—
            vector_results = self.vector_search(query, top_k * 2)
            bm25_results = self.keyword_search(query, top_k * 2)
            tfidf_results = self.tfidf_search(query, top_k * 2)
            
            logger.info(f"ğŸ“Š æ¤œç´¢çµæœæ•°: ãƒ™ã‚¯ãƒˆãƒ«={len(vector_results)}, BM25={len(bm25_results)}, TF-IDF={len(tfidf_results)}")
            
            # ã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–ã—ã¦ãƒãƒ¼ã‚¸
            all_candidates = {}
            keyword_weight = (1 - vector_weight) / 2
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ
            for idx, score in vector_results:
                all_candidates[idx] = all_candidates.get(idx, 0) + score * vector_weight
            
            # BM25çµæœ
            max_bm25 = max([score for _, score in bm25_results], default=1)
            for idx, score in bm25_results:
                normalized_score = score / max_bm25 if max_bm25 > 0 else 0
                all_candidates[idx] = all_candidates.get(idx, 0) + normalized_score * keyword_weight
            
            # TF-IDFçµæœ
            max_tfidf = max([score for _, score in tfidf_results], default=1)
            for idx, score in tfidf_results:
                normalized_score = score / max_tfidf if max_tfidf > 0 else 0
                all_candidates[idx] = all_candidates.get(idx, 0) + normalized_score * keyword_weight
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_candidates = sorted(all_candidates.items(), key=lambda x: x[1], reverse=True)
            candidate_indices = [idx for idx, _ in sorted_candidates[:top_k * 2]]
            
            logger.info(f"ğŸ”„ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å‰ã®å€™è£œæ•°: {len(candidate_indices)}")
            
            # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®šã®å•é¡Œã‚’è§£æ±º - ã‚ˆã‚Šå¤šãã®çµæœã‚’è¿”ã™ã‚ˆã†ã«
            final_top_k = min(top_k, 80)  # 40ã‹ã‚‰80ã«å¢—åŠ 
            reranked_indices = self.rerank_results(query, candidate_indices, final_top_k)
            
            logger.info(f"âœ… ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¾Œã®çµæœæ•°: {len(reranked_indices)} (æœ€å¤§{final_top_k})")
            
            # çµæœã‚’æ•´å½¢
            results = []
            bunya_counts = {}
            
            for idx in reranked_indices:
                item = self.df.iloc[idx]
                bunya = item['bunya_name']
                bunya_counts[bunya] = bunya_counts.get(bunya, 0) + 1
                
                results.append({
                    'koumoku_name': item.get('koumoku_name', item['koumoku_name_full']),
                    'koumoku_name_full': item['koumoku_name_full'],
                    'bunya_name': item['bunya_name'],
                    'chuubunrui_name': item['chuubunrui_name'],
                    'shoubunrui_name': item['shoubunrui_name'],
                    'score': all_candidates.get(idx, 0)
                })
            
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœã®åˆ†é‡åˆ†å¸ƒ: {dict(bunya_counts)}")
            
            # è©³ç´°çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"ğŸ“‹ æ¤œç´¢çµæœè©³ç´°:")
            for i, result in enumerate(results, 1):
                logger.info(f"  {i:2d}. {result['koumoku_name_full']} ({result['bunya_name']})")
            
            logger.info(f"ğŸ¯ æ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®æŒ‡æ¨™ã‚’è¿”å´")
            
            return results
            
        except Exception as e:
            st.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
retriever = HybridRetriever() 