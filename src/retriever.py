import pandas as pd
import numpy as np
import faiss
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st
import json
import os
from typing import List, Dict, Tuple, Optional

from encoder import EmbeddingConfig
from loguru import logger

class HybridRetriever:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰ã¨ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.df = None
        self.faiss_index = None
        self.bm25 = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.metadata = None
    
    def load_vector_database(self, vector_db_dir: str = '../vector_db') -> bool:
        """ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            # æ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if (self.df is not None and hasattr(self, 'index') and self.index is not None 
                and hasattr(self, 'bm25_index') and self.bm25_index is not None):
                logger.info("ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯æ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿")
                return True
                
            logger.info(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿é–‹å§‹: {vector_db_dir}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            metadata_path = os.path.join(vector_db_dir, 'metadata.json')
            if not os.path.exists(metadata_path):
                logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_path}")
                return False
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
            
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            parquet_path = os.path.join(vector_db_dir, 'processed_data.parquet')
            if not os.path.exists(parquet_path):
                logger.error(f"âŒ Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {parquet_path}")
                return False
            
            self.df = pd.read_parquet(parquet_path)
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)}è¡Œ")
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
            index_path = os.path.join(vector_db_dir, 'faiss_index.bin')
            if not os.path.exists(index_path):
                logger.error(f"âŒ FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {index_path}")
                return False
            
            self.index = faiss.read_index(index_path)
            logger.info(f"âœ… FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿å®Œäº†: {self.index.ntotal}ä»¶")
            
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šã‚’åˆæœŸåŒ–
            self.embedding_config = EmbeddingConfig()
            logger.info(f"âœ… ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šåˆæœŸåŒ–å®Œäº†")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
            self._build_keyword_indices()
            logger.info(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†")
            
            logger.info(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿å…¨ä½“å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            logger.error(f"âŒ ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
            return False
    
    def _build_keyword_indices(self):
        """BM25ã¨TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        try:
            # æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
            search_texts = []
            for _, row in self.df.iterrows():
                text = f"{row['koumoku_name_full']} {row['bunya_name']} {row['chuubunrui_name']} {row['shoubunrui_name']} {row['definition']} {row['stat_name']}"
                search_texts.append(text)
            
            # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            tokenized_texts = [text.split() for text in search_texts]
            self.bm25 = BM25Okapi(tokenized_texts)
            
            # TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),
                stop_words=None  # æ—¥æœ¬èªå¯¾å¿œã®ãŸã‚
            )
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(search_texts)
            
        except Exception as e:
            st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def vector_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            if self.index is None or self.embedding_config is None:
                logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¾ãŸã¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šãŒæœªåˆæœŸåŒ–")
                return []
            
            # ã‚¯ã‚¨ãƒªã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—
            query_embedding = self.embedding_config.get_embeddings([query])
            
            if query_embedding.size == 0:
                logger.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # FAISSæ¤œç´¢
            distances, indices = self.index.search(query_embedding, top_k)
            
            # çµæœã‚’æ•´å½¢
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx != -1:  # æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    similarity = 1.0 / (1.0 + distance)  # è·é›¢ã‚’é¡ä¼¼åº¦ã«å¤‰æ›
                    results.append((idx, similarity))
            
            logger.info(f"ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def keyword_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            if self.bm25 is None:
                logger.error("âŒ BM25æ¤œç´¢: BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªåˆæœŸåŒ–")
                return []
                
            query_tokens = query.split()
            bm25_scores = self.bm25.get_scores(query_tokens)
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            indexed_scores = [(i, score) for i, score in enumerate(bm25_scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            results = indexed_scores[:top_k]
            logger.info(f"ğŸ” BM25æ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def tfidf_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """TF-IDFæ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            if self.tfidf_vectorizer is None or self.tfidf_matrix is None:
                logger.error("âŒ TF-IDFæ¤œç´¢: TF-IDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªåˆæœŸåŒ–")
                return []
                
            query_vector = self.tfidf_vectorizer.transform([query])
            cosine_scores = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
            
            indexed_scores = [(i, score) for i, score in enumerate(cosine_scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)
            
            results = indexed_scores[:top_k]
            logger.info(f"ğŸ” TF-IDFæ¤œç´¢å®Œäº†: {len(results)}ä»¶ (è¦æ±‚:{top_k}ä»¶)")
            return results
            
        except Exception as e:
            logger.error(f"âŒ TF-IDFæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.error(f"TF-IDFæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def rerank_results(self, query: str, candidate_indices: List[int], top_k: int = 50) -> List[int]:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰"""
        try:
            query_lower = query.lower()
            scored_candidates = []
            
            for idx in candidate_indices:
                row = self.df.iloc[idx]
                
                # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã®ãƒãƒƒãƒãƒ³ã‚°åº¦ã‚’è¨ˆç®—
                score = 0
                text_fields = [
                    row['koumoku_name_full'],
                    row['bunya_name'],
                    row['chuubunrui_name'],
                    row['shoubunrui_name'],
                    row['definition'],
                    row['stat_name']
                ]
                
                for field in text_fields:
                    if pd.isna(field):
                        continue
                    field_lower = str(field).lower()
                    
                    # å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
                    if query_lower in field_lower:
                        score += 2
                    
                    # éƒ¨åˆ†ä¸€è‡´
                    query_words = query_lower.split()
                    field_words = field_lower.split()
                    matches = len(set(query_words) & set(field_words))
                    score += matches
                
                scored_candidates.append((idx, score))
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™
            scored_candidates.sort(key=lambda x: x[1], reverse=True)
            return [idx for idx, _ in scored_candidates[:top_k]]
            
        except Exception as e:
            st.error(f"ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return candidate_indices[:top_k]
    
    def hybrid_search(self, query: str, top_k: int = 50, vector_weight: float = 0.6) -> List[Dict]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸ” ã‚¯ã‚¨ãƒª: '{query}' (top_k={top_k})")
            
            # å„æ¤œç´¢æ‰‹æ³•ã‚’å®Ÿè¡Œ
            # å„æ¤œç´¢æ‰‹æ³•ã§çµæœã‚’å–å¾—
            vector_results = self.vector_search(query, top_k * 2)
            bm25_results = self.keyword_search(query, top_k * 2)
            tfidf_results = self.tfidf_search(query, top_k * 2)
            
            logger.info(f"ğŸ“Š æ¤œç´¢çµæœæ•°: ãƒ™ã‚¯ãƒˆãƒ«={len(vector_results)}, BM25={len(bm25_results)}, TF-IDF={len(tfidf_results)}")
            
            # ã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–ã—ã¦ãƒãƒ¼ã‚¸
            all_candidates = {}
            keyword_weight = (1 - vector_weight) / 2
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ
            for idx, score in vector_results:
                all_candidates[idx] = all_candidates.get(idx, 0) + score * vector_weight
            
            # BM25çµæœ
            max_bm25 = max([score for _, score in bm25_results], default=1)
            for idx, score in bm25_results:
                normalized_score = score / max_bm25 if max_bm25 > 0 else 0
                all_candidates[idx] = all_candidates.get(idx, 0) + normalized_score * keyword_weight
            
            # TF-IDFçµæœ
            max_tfidf = max([score for _, score in tfidf_results], default=1)
            for idx, score in tfidf_results:
                normalized_score = score / max_tfidf if max_tfidf > 0 else 0
                all_candidates[idx] = all_candidates.get(idx, 0) + normalized_score * keyword_weight
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_candidates = sorted(all_candidates.items(), key=lambda x: x[1], reverse=True)
            candidate_indices = [idx for idx, _ in sorted_candidates[:top_k * 2]]
            
            logger.info(f"ğŸ”„ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å‰ã®å€™è£œæ•°: {len(candidate_indices)}")
            
            # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®šã®å•é¡Œã‚’è§£æ±º - ã‚ˆã‚Šå¤šãã®çµæœã‚’è¿”ã™ã‚ˆã†ã«
            final_top_k = min(top_k, 80)  # 40ã‹ã‚‰80ã«å¢—åŠ 
            reranked_indices = self.rerank_results(query, candidate_indices, final_top_k)
            
            logger.info(f"âœ… ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¾Œã®çµæœæ•°: {len(reranked_indices)} (æœ€å¤§{final_top_k})")
            
            # çµæœã‚’æ•´å½¢
            results = []
            bunya_counts = {}
            
            for idx in reranked_indices:
                item = self.df.iloc[idx]
                bunya = item['bunya_name']
                bunya_counts[bunya] = bunya_counts.get(bunya, 0) + 1
                
                results.append({
                    'koumoku_name': item.get('koumoku_name', item['koumoku_name_full']),
                    'koumoku_name_full': item['koumoku_name_full'],
                    'bunya_name': item['bunya_name'],
                    'chuubunrui_name': item['chuubunrui_name'],
                    'shoubunrui_name': item['shoubunrui_name'],
                    'score': all_candidates.get(idx, 0)
                })
            
            logger.info(f"ğŸ“ˆ æœ€çµ‚çµæœã®åˆ†é‡åˆ†å¸ƒ: {dict(bunya_counts)}")
            
            # è©³ç´°çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"ğŸ“‹ æ¤œç´¢çµæœè©³ç´°:")
            for i, result in enumerate(results, 1):
                logger.info(f"  {i:2d}. {result['koumoku_name_full']} ({result['bunya_name']})")
            
            logger.info(f"ğŸ¯ æ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®æŒ‡æ¨™ã‚’è¿”å´")
            
            return results
            
        except Exception as e:
            st.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
retriever = HybridRetriever() 