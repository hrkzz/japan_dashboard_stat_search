import os
from litellm import completion
from typing import Optional, Dict, Any
import ollama
from config import config

class LLMConfig:
    """LLMã®è¨­å®šã¨åˆæœŸåŒ–ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.api_keys = {}
        self.current_model = None
        self.ollama_models = []
        self.setup_api_keys()
    
    def setup_api_keys(self):
        """åˆ©ç”¨å¯èƒ½ãªAPIã‚­ãƒ¼ã‚’ã™ã¹ã¦è¨­å®š"""
        self.api_keys = self._get_api_keys()
        
        # Ollamaãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã‚’è©¦è¡Œ
        self._fetch_ollama_models()
        
        # ãƒ‡ãƒãƒƒã‚°: APIã‚­ãƒ¼ã®å–å¾—çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
        print(f"ğŸ”§ å–å¾—ã—ãŸAPIã‚­ãƒ¼: {list(self.api_keys.keys())}")
        for key_type, key_value in self.api_keys.items():
            print(f"  {key_type}: {'è¨­å®šæ¸ˆã¿' if key_value else 'æœªè¨­å®š'} ({len(key_value) if key_value else 0}æ–‡å­—)")
        
        # Ollamaãƒ¢ãƒ‡ãƒ«ã®å–å¾—çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
        if self.ollama_models:
            print(f"ğŸ¦™ Ollamaåˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {[model['name'] for model in self.ollama_models]}")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šï¼ˆOpenAIã‚’å„ªå…ˆã«å¤‰æ›´ï¼‰
        if self.api_keys.get('openai'):
            self.current_model = "gpt-4o-mini"
            print("ğŸš€ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: OpenAI GPT-4o-mini")
        elif self.api_keys.get('gemini'):
            self.current_model = "gemini-2.0-flash-exp"
            print("ğŸš€ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: Gemini 2.0 Flash")
        elif self.ollama_models:
            self.current_model = f"ollama/{self.ollama_models[0]['name']}"
            print(f"ğŸš€ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: Ollama {self.ollama_models[0]['name']}")
        else:
            self.current_model = None
            print("âŒ åˆ©ç”¨å¯èƒ½ãªAPIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")
    
    def _fetch_ollama_models(self):
        """Ollamaã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        if not self.api_keys.get('ollama'):
            return
        
        try:
            # Ollama ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            client = ollama.Client(host=self.api_keys['ollama'])
            models_response = client.list()
            
            # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜
            if 'models' in models_response:
                self.ollama_models = models_response['models']
                print(f"âœ… Ollamaæ¥ç¶šæˆåŠŸ: {len(self.ollama_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½")
            else:
                print("âš ï¸ Ollamaã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.ollama_models = []
    
    def get_available_models(self) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        available_models = {}
        
        if self.api_keys.get('openai'):
            available_models["OpenAI GPT-4o-mini"] = "gpt-4o-mini"
        
        if self.api_keys.get('gemini'):
            available_models["Google Gemini 2.0 Flash"] = "gemini-2.0-flash-exp"
        
        # Ollamaãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
        for model in self.ollama_models:
            model_name = model['name']
            display_name = f"Ollama: {model_name}"
            available_models[display_name] = f"ollama/{model_name}"
        
        return available_models
    
    def set_model(self, model_name: str):
        """ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
        self.current_model = model_name
        
        # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        if model_name.startswith("gpt"):
            os.environ["OPENAI_API_KEY"] = self.api_keys['openai']
        elif model_name.startswith("gemini"):
            os.environ["GEMINI_API_KEY"] = self.api_keys['gemini']
        # Ollamaã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°è¨­å®šä¸è¦ï¼ˆapi_baseã§ç›´æ¥æŒ‡å®šï¼‰
    
    def _get_api_keys(self):
        """APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•° > secrets.toml ã‚’ `Config` ã‹ã‚‰ï¼‰"""
        return {
            'openai': config.get_openai_key(),
            'gemini': config.get_gemini_key(),
            'ollama': config.get_ollama_base_url(),
        }
    
    def generate_response(self, messages: list, temperature: float = 0.3) -> str:
        """LLMã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        if not self.current_model:
            return "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ãƒ¢ãƒ‡ãƒ«åã‚’litellmå½¢å¼ã«å¤‰æ›
            litellm_model = self.current_model
            if self.current_model.startswith("gemini"):
                litellm_model = f"gemini/{self.current_model}"
            
            # completioné–¢æ•°ã®å¼•æ•°ã‚’æº–å‚™
            completion_args = {
                "model": litellm_model,
                "messages": messages,
                "temperature": temperature
            }
            
            # Ollamaãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯api_baseã‚’è¿½åŠ 
            if self.current_model.startswith('ollama/'):
                completion_args["api_base"] = self.api_keys['ollama']
            
            response = completion(**completion_args)
            return response.choices[0].message.content
        except Exception as e:
            return f"LLMå¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
llm_config = LLMConfig()