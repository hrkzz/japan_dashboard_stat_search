import os
import streamlit as st
from litellm import completion
from typing import Optional, Dict, Any

class LLMConfig:
    """LLMã®è¨­å®šã¨åˆæœŸåŒ–ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.api_keys = {}
        self.current_model = None
        self.setup_api_keys()
    
    def setup_api_keys(self):
        """åˆ©ç”¨å¯èƒ½ãªAPIã‚­ãƒ¼ã‚’ã™ã¹ã¦è¨­å®š"""
        self.api_keys = self._get_api_keys()
        
        # ãƒ‡ãƒãƒƒã‚°: APIã‚­ãƒ¼ã®å–å¾—çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
        print(f"ğŸ”§ å–å¾—ã—ãŸAPIã‚­ãƒ¼: {list(self.api_keys.keys())}")
        for key_type, key_value in self.api_keys.items():
            print(f"  {key_type}: {'è¨­å®šæ¸ˆã¿' if key_value else 'æœªè¨­å®š'} ({len(key_value) if key_value else 0}æ–‡å­—)")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šï¼ˆOpenAIã‚’å„ªå…ˆã«å¤‰æ›´ï¼‰
        if self.api_keys.get('openai'):
            self.current_model = "gpt-4o-mini"
            print("ğŸš€ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: OpenAI GPT-4o-mini")
        elif self.api_keys.get('gemini'):
            self.current_model = "gemini-2.0-flash-exp"
            print("ğŸš€ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: Gemini 2.0 Flash")
        else:
            self.current_model = None
            print("âŒ åˆ©ç”¨å¯èƒ½ãªAPIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")
    
    def get_available_models(self) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        available_models = {}
        
        if self.api_keys.get('openai'):
            available_models["OpenAI GPT-4o-mini"] = "gpt-4o-mini"
        
        if self.api_keys.get('gemini'):
            available_models["Google Gemini 2.0 Flash"] = "gemini-2.0-flash-exp"
        
        return available_models
    
    def set_model(self, model_name: str):
        """ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
        self.current_model = model_name
        
        # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        if model_name.startswith("gpt"):
            os.environ["OPENAI_API_KEY"] = self.api_keys['openai']
        elif model_name.startswith("gemini"):
            os.environ["GEMINI_API_KEY"] = self.api_keys['gemini']
    
    def _get_api_keys(self):
        """APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆStreamlit secrets.toml ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ï¼‰"""
        api_keys = {}
        
        try:
            # Streamlitå†…ã§ã®å®Ÿè¡Œã®å ´åˆ
            if "OPENAI_API_KEY" in st.secrets:
                api_keys['openai'] = st.secrets["OPENAI_API_KEY"]
            if "GEMINI_API_KEY" in st.secrets:
                api_keys['gemini'] = st.secrets["GEMINI_API_KEY"]
            if "OLLAMA_BASE_URL" in st.secrets:
                api_keys['ollama'] = st.secrets["OLLAMA_BASE_URL"]
        except:
            # Streamlitå¤–ã§ã®å®Ÿè¡Œã®å ´åˆã€ç›´æ¥secrets.tomlãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚€
            try:
                import toml
                secrets_path = os.path.join(os.path.dirname(__file__), '..', '.streamlit', 'secrets.toml')
                if os.path.exists(secrets_path):
                    with open(secrets_path, 'r') as f:
                        secrets = toml.load(f)
                    if 'OPENAI_API_KEY' in secrets:
                        api_keys['openai'] = secrets['OPENAI_API_KEY']
                    if 'GEMINI_API_KEY' in secrets:
                        api_keys['gemini'] = secrets['GEMINI_API_KEY']
                    if 'OLLAMA_BASE_URL' in secrets:
                        api_keys['ollama'] = secrets['OLLAMA_BASE_URL']
            except ImportError:
                print("tomlãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            except Exception as e:
                print(f"secrets.tomlèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ç’°å¢ƒå¤‰æ•°ã‚‚ãƒã‚§ãƒƒã‚¯
        if not api_keys.get('openai') and os.getenv('OPENAI_API_KEY'):
            api_keys['openai'] = os.getenv('OPENAI_API_KEY')
        if not api_keys.get('gemini') and os.getenv('GEMINI_API_KEY'):
            api_keys['gemini'] = os.getenv('GEMINI_API_KEY')
        if not api_keys.get('ollama') and os.getenv('OLLAMA_BASE_URL'):
            api_keys['ollama'] = os.getenv('OLLAMA_BASE_URL')
        
        return api_keys
    
    def generate_response(self, messages: list, temperature: float = 0.3) -> str:
        """LLMã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        if not self.current_model:
            return "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ãƒ¢ãƒ‡ãƒ«åã‚’litellmå½¢å¼ã«å¤‰æ›
            litellm_model = self.current_model
            if self.current_model.startswith("gemini"):
                litellm_model = f"gemini/{self.current_model}"
            
            response = completion(
                model=litellm_model,
                messages=messages,
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"LLMå¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
llm_config = LLMConfig() 