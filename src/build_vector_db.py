#!/usr/bin/env python3
"""
ç·å‹™çœçµ±è¨ˆå±€ ç¤¾ä¼šãƒ»äººå£çµ±è¨ˆä½“ç³»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰
ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python build_vector_db.py
"""

import pandas as pd
import faiss
import numpy as np
import os
import json
from datetime import datetime
import argparse
import joblib
from encoder import embedding_config

def verify_api_setup():
    """APIè¨­å®šã‚’ç¢ºèª"""
    try:
        test_embedding = embedding_config.get_single_embedding("ãƒ†ã‚¹ãƒˆ")
        if test_embedding.size == 0:
            raise ValueError("ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("âœ… APIè¨­å®šç¢ºèªå®Œäº†")
        return True
    except Exception as e:
        print(f"âŒ APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        print("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY / GEMINI_API_KEY ã¾ãŸã¯ OLLAMA_BASE_URL ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return False

def load_and_preprocess_data():
    """CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’è¡Œã†"""
    try:
        print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        mst_df = pd.read_csv('../data/social_demographic_stat_mst_koumoku.csv')
        def_df = pd.read_csv('../data/social_demographic_stat_def_koumoku.csv')
        
        print(f"   ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿: {len(mst_df):,}è¡Œ")
        print(f"   å®šç¾©ãƒ‡ãƒ¼ã‚¿: {len(def_df):,}è¡Œ")
        
        # PowerBIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã«å¿…è¦ãªåˆ—ã‚’é¸æŠ
        mst_selected = mst_df[['koumoku_code', 'koumoku_name', 'koumoku_name_full', 'bunya_name', 'chuubunrui_name', 'shoubunrui_name', 'stat_name']].copy()
        def_selected = def_df[['koumoku_code', 'definition']].copy()
        
        # å®šç¾©ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆLEFT JOINï¼‰
        df = mst_selected.merge(def_selected, on='koumoku_code', how='left')
        
        # NaNå€¤ã®å‡¦ç†
        df['definition'] = df['definition'].fillna('')
        
        # æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        print("ğŸ”¤ æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...")
        df['search_text'] = df.apply(lambda row: f"{row['koumoku_name_full']} {row['bunya_name']} {row['chuubunrui_name']} {row['shoubunrui_name']} {row['definition']} {row['stat_name']}", axis=1)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†: {len(df):,}ä»¶")
        return df
        
    except FileNotFoundError as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
        print("   data/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return None
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if pd.isna(text) or text is None:
        return "çµ±è¨ˆæŒ‡æ¨™"
    
    # æ–‡å­—åˆ—ã«å¤‰æ›
    text = str(text).strip()
    
    # ç©ºæ–‡å­—ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if len(text) == 0:
        return "çµ±è¨ˆæŒ‡æ¨™"
    
    return text.strip()

def create_embeddings(texts, batch_size=100):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’Embeddingã«å¤‰æ›"""
    print(f"ğŸ§  Embeddingã‚’ç”Ÿæˆä¸­... ({len(texts):,}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ)")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    print("   ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    cleaned_texts = [clean_text(text) for text in texts]
    
    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
    empty_count = sum(1 for text in cleaned_texts if not text or len(text.strip()) == 0)
    if empty_count > 0:
        print(f"   è­¦å‘Š: {empty_count}ä»¶ã®ç©ºãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ç½®æ›ã—ã¾ã—ãŸ")
    
    embeddings = []
    
    for i in range(0, len(cleaned_texts), batch_size):
        batch = cleaned_texts[i:i + batch_size]
        print(f"   é€²æ—: {i + len(batch):,} / {len(cleaned_texts):,}")
        
        # ãƒãƒƒãƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å†åº¦æ¤œè¨¼
        valid_batch = []
        for text in batch:
            if text and len(text.strip()) > 0:
                valid_batch.append(text)
            else:
                valid_batch.append("çµ±è¨ˆæŒ‡æ¨™")
        
        try:
            batch_embeddings_array = embedding_config.get_embeddings(valid_batch)
            if batch_embeddings_array.size > 0:
                embeddings.extend(batch_embeddings_array.tolist())
            else:
                print(f"âŒ ãƒãƒƒãƒ {i//batch_size + 1}ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«å¤±æ•—")
                return None
            
        except Exception as e:
            print(f"âŒ Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i//batch_size + 1}): {str(e)}")
            return None
    
    print("âœ… Embeddingç”Ÿæˆå®Œäº†")
    return np.array(embeddings, dtype=np.float32)

def build_faiss_index(embeddings):
    """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
    print("ğŸ” FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¬¡å…ƒæ•°
    dimension = embeddings.shape[1]
    print(f"   æ¬¡å…ƒæ•°: {dimension}")
    
    # L2æ­£è¦åŒ–
    faiss.normalize_L2(embeddings)
    
    # IndexFlatIPã‚’ä½¿ç”¨ï¼ˆå†…ç©ã«ã‚ˆã‚‹é¡ä¼¼åº¦æ¤œç´¢ï¼‰
    index = faiss.IndexFlatIP(dimension)
    index.add(embeddings)
    
    print(f"âœ… FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {index.ntotal:,}ä»¶")
    return index

def save_database(df, faiss_index, output_dir='../vector_db', bm25=None, tfidf_vectorizer=None, tfidf_matrix=None):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ä¸­... ({output_dir})")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # DataFrameä¿å­˜
    print("   DataFrameã‚’ä¿å­˜ä¸­...")
    df.to_parquet(f"{output_dir}/processed_data.parquet", index=False)
    
    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
    print("   FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ä¸­...")
    faiss.write_index(faiss_index, f"{output_dir}/faiss_index.bin")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        'created_at': datetime.now().isoformat(),
        'total_records': len(df),
        'embedding_model': embedding_config.embedding_model or embedding_config._get_embedding_model(),
        'vector_dimension': faiss_index.d
    }
    
    with open(f"{output_dir}/metadata.json", 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    # è¿½åŠ : BM25/TF-IDF ã‚’æ°¸ç¶šåŒ–
    if bm25 is not None:
        print("   BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ä¸­...")
        joblib.dump(bm25, f"{output_dir}/bm25.joblib")
    if tfidf_vectorizer is not None and tfidf_matrix is not None:
        print("   TF-IDFãƒ™ã‚¯ã‚¿ã‚¤ã‚¶/è¡Œåˆ—ã‚’ä¿å­˜ä¸­...")
        joblib.dump({"vectorizer": tfidf_vectorizer, "matrix": tfidf_matrix}, f"{output_dir}/tfidf.joblib")
    
    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†")

def main():
    parser = argparse.ArgumentParser(description='çµ±è¨ˆæŒ‡æ¨™ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰')
    parser.add_argument('--batch-size', type=int, default=100, help='Embeddingç”Ÿæˆã®ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--output-dir', default='../vector_db', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    args = parser.parse_args()
    
    print("ğŸš€ ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚’é–‹å§‹...")
    print("=" * 50)
    
    # APIè¨­å®šã®ç¢ºèª
    if not verify_api_setup():
        return
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
    df = load_and_preprocess_data()
    if df is None:
        return
    
    # Embeddingã®ç”Ÿæˆ
    search_texts = df['search_text'].tolist()
    embeddings = create_embeddings(search_texts, args.batch_size)
    if embeddings is None:
        return
    
    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰
    faiss_index = build_faiss_index(embeddings)
    
    # è¿½åŠ : BM25/TF-IDF ã‚’äº‹å‰è¨ˆç®—
    try:
        from rank_bm25 import BM25Okapi
        from sklearn.feature_extraction.text import TfidfVectorizer
        print("   BM25/TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
        tokenized_texts = [text.split() for text in search_texts]
        bm25 = BM25Okapi(tokenized_texts)
        tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words=None)
        tfidf_matrix = tfidf_vectorizer.fit_transform(search_texts)
    except Exception as e:
        print(f"âš ï¸ BM25/TF-IDF æ§‹ç¯‰ã«å¤±æ•—: {e}")
        bm25, tfidf_vectorizer, tfidf_matrix = None, None, None

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¿å­˜ï¼ˆBM25/TF-IDF ã‚‚å«ã‚ã‚‹ï¼‰
    save_database(df, faiss_index, args.output_dir, bm25=bm25, tfidf_vectorizer=tfidf_vectorizer, tfidf_matrix=tfidf_matrix)
    
    print("=" * 50)
    print("ğŸ‰ ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº†ï¼")
    print(f"   ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}ä»¶")
    print(f"   ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {embeddings.shape[1]}")
    print(f"   ä¿å­˜å…ˆ: {args.output_dir}")

if __name__ == "__main__":
    main() 