#!/usr/bin/env python3
"""
OpenAI GPT-4o-mini vs Google Gemini 2.0 Flash é€Ÿåº¦æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
"""

import sys
import os
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆsrc ã‚’ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦è§£æ±ºã™ã‚‹ãŸã‚ï¼‰
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import time
from loguru import logger
from services import AnalysisService
from llm_config import llm_config

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stdout, level='INFO', format='{time:HH:mm:ss} | {level} | {message}')

def test_model_speed(model_name: str, model_display_name: str, test_query: str) -> dict:
    """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®é€Ÿåº¦ãƒ†ã‚¹ãƒˆ"""
    logger.info(f"ğŸš€ {model_display_name} ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ
    llm_config.set_model(model_name)
    
    # 3å›ãƒ†ã‚¹ãƒˆã—ã¦å¹³å‡ã‚’å–ã‚‹
    times = []
    results = []
    
    for i in range(3):
        logger.info(f"  ãƒ†ã‚¹ãƒˆ {i+1}/3 å®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        try:
            service = AnalysisService()
            result = service.generate_ai_analysis(test_query)
            end_time = time.time()
            
            if result and 'analysis_perspectives' in result:
                test_time = end_time - start_time
                times.append(test_time)
                total_indicators = sum(len(p['recommended_indicators']) for p in result['analysis_perspectives'])
                results.append(total_indicators)
                logger.info(f"    ãƒ†ã‚¹ãƒˆ {i+1}: {test_time:.2f}ç§’, æŒ‡æ¨™æ•°: {total_indicators}ä»¶")
            else:
                logger.error(f"    ãƒ†ã‚¹ãƒˆ {i+1}: å¤±æ•—")
                times.append(None)
                results.append(0)
        except Exception as e:
            logger.error(f"    ãƒ†ã‚¹ãƒˆ {i+1}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")
            times.append(None)
            results.append(0)
    
    # çµæœã‚’è¨ˆç®—
    valid_times = [t for t in times if t is not None]
    avg_time = sum(valid_times) / len(valid_times) if valid_times else None
    avg_indicators = sum(results) / len(results) if results else 0
    
    return {
        'model': model_display_name,
        'times': times,
        'avg_time': avg_time,
        'avg_indicators': avg_indicators,
        'success_rate': len(valid_times) / len(times) * 100
    }

def main():
    logger.info("=" * 60)
    logger.info("ğŸ OpenAI vs Gemini é€Ÿåº¦æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "å­è‚²ã¦ç’°å¢ƒã‚’æ¯”è¼ƒã—ãŸã„",
        "åœ°åŸŸã®æ•™è‚²æ°´æº–ã‚’çŸ¥ã‚ŠãŸã„",
        "é«˜é½¢åŒ–ã®ç¾çŠ¶ã‚’æŠŠæ¡ã—ãŸã„"
    ]
    
    # åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«å–å¾—
    available_models = llm_config.get_available_models()
    logger.info(f"åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {list(available_models.keys())}")
    
    if len(available_models) < 2:
        logger.error("æ¯”è¼ƒã™ã‚‹ã«ã¯2ã¤ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™")
        return
    
    results = []
    
    # å„ã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆ
    for query_idx, query in enumerate(test_queries, 1):
        logger.info(f"\nğŸ“Š ã‚¯ã‚¨ãƒª {query_idx}/3: '{query}'")
        logger.info("-" * 40)
        
        query_results = []
        
        # å„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
        for display_name, model_name in available_models.items():
            result = test_model_speed(model_name, display_name, query)
            query_results.append(result)
            
            if result['avg_time']:
                logger.info(f"âœ… {display_name}: å¹³å‡ {result['avg_time']:.2f}ç§’")
            else:
                logger.error(f"âŒ {display_name}: ãƒ†ã‚¹ãƒˆå¤±æ•—")
        
        results.append({
            'query': query,
            'results': query_results
        })
    
    # ç·åˆçµæœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“ˆ ç·åˆçµæœ")
    logger.info("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®å¹³å‡è¨ˆç®—
    model_stats = {}
    for model_display, _ in available_models.items():
        times = []
        indicators = []
        success_count = 0
        
        for query_result in results:
            for model_result in query_result['results']:
                if model_result['model'] == model_display:
                    if model_result['avg_time']:
                        times.append(model_result['avg_time'])
                        indicators.append(model_result['avg_indicators'])
                        success_count += 1
        
        if times:
            model_stats[model_display] = {
                'avg_time': sum(times) / len(times),
                'avg_indicators': sum(indicators) / len(indicators),
                'success_rate': success_count / len(test_queries) * 100
            }
    
    # çµæœè¡¨ç¤º
    for model, stats in model_stats.items():
        logger.info(f"\nğŸ¤– {model}:")
        logger.info(f"  â±ï¸  å¹³å‡å‡¦ç†æ™‚é–“: {stats['avg_time']:.2f}ç§’")
        logger.info(f"  ğŸ“Š å¹³å‡æŒ‡æ¨™æ•°: {stats['avg_indicators']:.1f}ä»¶")
        logger.info(f"  âœ… æˆåŠŸç‡: {stats['success_rate']:.0f}%")
    
    # å‹è€…åˆ¤å®š
    if len(model_stats) >= 2:
        fastest = min(model_stats.items(), key=lambda x: x[1]['avg_time'])
        logger.info(f"\nğŸ† æœ€é«˜é€Ÿ: {fastest[0]} ({fastest[1]['avg_time']:.2f}ç§’)")
        
        # é€Ÿåº¦å·®è¨ˆç®—
        times_list = [(name, stats['avg_time']) for name, stats in model_stats.items()]
        times_list.sort(key=lambda x: x[1])
        
        if len(times_list) >= 2:
            speed_improvement = ((times_list[1][1] - times_list[0][1]) / times_list[1][1] * 100)
            logger.info(f"ğŸš€ é€Ÿåº¦å·®: {speed_improvement:.1f}%é«˜é€ŸåŒ–")

if __name__ == "__main__":
    main() 